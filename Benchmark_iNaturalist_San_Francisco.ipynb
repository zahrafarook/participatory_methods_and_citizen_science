{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a85c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from base64 import b64decode\n",
    "from json import loads\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# set matplotlib to display all plots inline with the notebook\n",
    "%matplotlib inline\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738dc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_years = {\n",
    " 'Francisco_Bay':[2016,2017,2018,2019,2020,2021,2022,2023],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb3c46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cnc', 'Virginia', '.DS_Store', 'San_Franciso', 'France', 'San_Francisco_Bay_2020.csv', 'San_Francisco_Bay_2021.csv', 'census_ethnicities', 'San_Francisco_Bay_2023.csv', 'San_Francisco_Bay_2022.csv', 'Sri_Lanka', 'Los_Angeles_2017.csv', 'San_Francisco_Bay_2019.csv', 'San_Francisco_Bay_2018.csv', 'non_cnc', 'Bolivia', 'San_Francisco_Bay_2016.csv', 'northcarolina', 'UK', 'San_Francisco_Bay_2017.csv', 'Los_Angeles', 'India', 'Texas', 'old data']\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/zahrafarook/Desktop/DataAnalysis/data'\n",
    "dir_list = os.listdir(path)\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8fac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  Francisco_Bay 2016\n",
      "Loading:  Francisco_Bay 2017\n",
      "Loading:  Francisco_Bay 2018\n",
      "Loading:  Francisco_Bay 2019\n",
      "Loading:  Francisco_Bay 2020\n",
      "Loading:  Francisco_Bay 2021\n",
      "Loading:  Francisco_Bay 2022\n",
      "Loading:  Francisco_Bay 2023\n",
      "Index(['id', 'observed_on_string', 'observed_on', 'time_observed_at',\n",
      "       'created_time_zone', 'created_at', 'updated_at', 'description',\n",
      "       'user_id', 'quality_grade', 'reviewed_by', 'faves_count',\n",
      "       'num_identification_agreements', 'num_identification_disagreements',\n",
      "       'identifications_most_agree', 'identifications_most_disagree',\n",
      "       'captive', 'place_guess', 'place_ids', 'longitude', 'latitude',\n",
      "       'positional_accuracy', 'geoprivacy', 'taxon_geoprivacy', 'obscured',\n",
      "       'species_guess', 'taxon_id', 'taxon_name', 'preferred_common_name',\n",
      "       'iconic_taxon_name', 'taxon_rank', 'taxon_parent_id', 'taxon_native',\n",
      "       'taxon_endemic', 'taxon_threatened', 'taxon_search_rank',\n",
      "       'taxon_observations', 'identifications'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>observed_on_string</th>\n",
       "      <th>observed_on</th>\n",
       "      <th>time_observed_at</th>\n",
       "      <th>created_time_zone</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>description</th>\n",
       "      <th>user_id</th>\n",
       "      <th>quality_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>preferred_common_name</th>\n",
       "      <th>iconic_taxon_name</th>\n",
       "      <th>taxon_rank</th>\n",
       "      <th>taxon_parent_id</th>\n",
       "      <th>taxon_native</th>\n",
       "      <th>taxon_endemic</th>\n",
       "      <th>taxon_threatened</th>\n",
       "      <th>taxon_search_rank</th>\n",
       "      <th>taxon_observations</th>\n",
       "      <th>identifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20069</td>\n",
       "      <td>1:15 pm.</td>\n",
       "      <td>2016-07-14</td>\n",
       "      <td>2016-07-14T13:15:00-07:00</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>2011-06-03T14:51:45-07:00</td>\n",
       "      <td>2020-02-09T08:18:18-08:00</td>\n",
       "      <td>Seen in grass.  Individual had &gt;5 ticks from e...</td>\n",
       "      <td>1704</td>\n",
       "      <td>casual</td>\n",
       "      <td>...</td>\n",
       "      <td>Western Alligator Lizards</td>\n",
       "      <td>Reptilia</td>\n",
       "      <td>genus</td>\n",
       "      <td>797512.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>36300.0</td>\n",
       "      <td>36300.0</td>\n",
       "      <td>[{'user_id': 1704, 'category': 'leading', 'dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20070</td>\n",
       "      <td>1:00 pm.</td>\n",
       "      <td>2016-03-25</td>\n",
       "      <td>2016-03-25T13:00:00-07:00</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>2011-06-03T14:53:13-07:00</td>\n",
       "      <td>2020-02-09T08:18:11-08:00</td>\n",
       "      <td>Female mallard duck with multiple ducklings.</td>\n",
       "      <td>1704</td>\n",
       "      <td>casual</td>\n",
       "      <td>...</td>\n",
       "      <td>Mallard</td>\n",
       "      <td>Aves</td>\n",
       "      <td>species</td>\n",
       "      <td>6922.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>462859.0</td>\n",
       "      <td>462859.0</td>\n",
       "      <td>[{'user_id': 642, 'category': 'leading', 'disa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68373</td>\n",
       "      <td>6:30</td>\n",
       "      <td>2016-02-12</td>\n",
       "      <td>2016-02-12T06:30:00-08:00</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>2012-04-20T20:36:48-07:00</td>\n",
       "      <td>2020-08-01T14:11:59-07:00</td>\n",
       "      <td>This flower has blue/purple petals that are di...</td>\n",
       "      <td>5844</td>\n",
       "      <td>casual</td>\n",
       "      <td>...</td>\n",
       "      <td>periwinkles</td>\n",
       "      <td>Plantae</td>\n",
       "      <td>genus</td>\n",
       "      <td>632394.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>98028.0</td>\n",
       "      <td>98028.0</td>\n",
       "      <td>[{'user_id': 5844, 'category': 'leading', 'dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158736</td>\n",
       "      <td>2:19</td>\n",
       "      <td>2016-10-14</td>\n",
       "      <td>2016-10-14T14:19:00-07:00</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>2012-12-06T20:23:52-08:00</td>\n",
       "      <td>2016-10-14T01:56:44-07:00</td>\n",
       "      <td>We saw our first devil's coach-horse beetle as...</td>\n",
       "      <td>11548</td>\n",
       "      <td>casual</td>\n",
       "      <td>...</td>\n",
       "      <td>Devil's Coach Horse Beetle</td>\n",
       "      <td>Insecta</td>\n",
       "      <td>species</td>\n",
       "      <td>1270925.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10365.0</td>\n",
       "      <td>10365.0</td>\n",
       "      <td>[{'user_id': 11548, 'category': 'leading', 'di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>538018</td>\n",
       "      <td>2016-04-10 2:20:00 PM PDT</td>\n",
       "      <td>2016-04-10</td>\n",
       "      <td>2016-04-10T14:20:00-07:00</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>2014-02-20T15:40:40-08:00</td>\n",
       "      <td>2016-04-10T21:06:27-07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16603</td>\n",
       "      <td>research</td>\n",
       "      <td>...</td>\n",
       "      <td>Western Fence Lizard</td>\n",
       "      <td>Reptilia</td>\n",
       "      <td>species</td>\n",
       "      <td>36141.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>119535.0</td>\n",
       "      <td>119535.0</td>\n",
       "      <td>[{'user_id': 16603, 'category': 'improving', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         observed_on_string observed_on           time_observed_at  \\\n",
       "0   20069                1:15 pm.     2016-07-14  2016-07-14T13:15:00-07:00   \n",
       "1   20070                1:00 pm.     2016-03-25  2016-03-25T13:00:00-07:00   \n",
       "2   68373                       6:30  2016-02-12  2016-02-12T06:30:00-08:00   \n",
       "3  158736                       2:19  2016-10-14  2016-10-14T14:19:00-07:00   \n",
       "4  538018  2016-04-10 2:20:00 PM PDT  2016-04-10  2016-04-10T14:20:00-07:00   \n",
       "\n",
       "     created_time_zone                 created_at                 updated_at  \\\n",
       "0  America/Los_Angeles  2011-06-03T14:51:45-07:00  2020-02-09T08:18:18-08:00   \n",
       "1  America/Los_Angeles  2011-06-03T14:53:13-07:00  2020-02-09T08:18:11-08:00   \n",
       "2  America/Los_Angeles  2012-04-20T20:36:48-07:00  2020-08-01T14:11:59-07:00   \n",
       "3  America/Los_Angeles  2012-12-06T20:23:52-08:00  2016-10-14T01:56:44-07:00   \n",
       "4  America/Los_Angeles  2014-02-20T15:40:40-08:00  2016-04-10T21:06:27-07:00   \n",
       "\n",
       "                                         description  user_id quality_grade  \\\n",
       "0  Seen in grass.  Individual had >5 ticks from e...     1704        casual   \n",
       "1     Female mallard duck with multiple ducklings.       1704        casual   \n",
       "2  This flower has blue/purple petals that are di...     5844        casual   \n",
       "3  We saw our first devil's coach-horse beetle as...    11548        casual   \n",
       "4                                                NaN    16603      research   \n",
       "\n",
       "   ...       preferred_common_name  iconic_taxon_name  taxon_rank  \\\n",
       "0  ...   Western Alligator Lizards           Reptilia       genus   \n",
       "1  ...                     Mallard               Aves     species   \n",
       "2  ...                 periwinkles            Plantae       genus   \n",
       "3  ...  Devil's Coach Horse Beetle            Insecta     species   \n",
       "4  ...        Western Fence Lizard           Reptilia     species   \n",
       "\n",
       "   taxon_parent_id  taxon_native  taxon_endemic  taxon_threatened  \\\n",
       "0         797512.0         False          False             False   \n",
       "1           6922.0          True          False             False   \n",
       "2         632394.0         False          False             False   \n",
       "3        1270925.0         False          False             False   \n",
       "4          36141.0          True          False             False   \n",
       "\n",
       "  taxon_search_rank taxon_observations  \\\n",
       "0           36300.0            36300.0   \n",
       "1          462859.0           462859.0   \n",
       "2           98028.0            98028.0   \n",
       "3           10365.0            10365.0   \n",
       "4          119535.0           119535.0   \n",
       "\n",
       "                                     identifications  \n",
       "0  [{'user_id': 1704, 'category': 'leading', 'dis...  \n",
       "1  [{'user_id': 642, 'category': 'leading', 'disa...  \n",
       "2  [{'user_id': 5844, 'category': 'leading', 'dis...  \n",
       "3  [{'user_id': 11548, 'category': 'leading', 'di...  \n",
       "4  [{'user_id': 16603, 'category': 'improving', '...  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = defaultdict(dict)\n",
    "dfall = []\n",
    "\n",
    "\n",
    "for city, years in city_years.items():\n",
    "    for year in years:\n",
    "        print(\"Loading: \", city, year)\n",
    "\n",
    "        df = pd.read_csv(\"/Users/zahrafarook/Desktop/DataAnalysis/data/San_{}_{}.csv\".format(city, year))\n",
    "       \n",
    "        #creating a column to add weekend or weekday\n",
    "        #df['time_observed_at'] = pd.to_datetime(df['time_observed_at'], utc=True)\n",
    "        #df['day_of_week'] = df['time_observed_at'].dt.dayofweek\n",
    "        #df['day_type'] = df['time_observed_at'].apply(lambda x: 'Weekend' if x.weekday() in [5, 6] else 'Weekday')\n",
    "        dfs[city][year] = df\n",
    "        dfall.append(df)\n",
    "\n",
    "dfall = pd.concat(dfall)\n",
    "print(dfall.columns)\n",
    "dfs['Francisco_Bay'][2016].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2294ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7cbe44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_ident_per_users(dfcity,year):\n",
    "    \"\"\" Calculate the number of observations and identifications per user \n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "     - dfcity: pd.DataFrame\n",
    "         The observations data for one city.\n",
    "     - year: int\n",
    "         The year corresponding to the observations data.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "     - dfobsident: pd.DataFrame\n",
    "         Has the columns: user_id | year | n_obs | n_idents\n",
    "    \"\"\"\n",
    "    \n",
    "    n_obs = defaultdict(int)\n",
    "    n_idents = defaultdict(int)\n",
    "    \n",
    "    for _, row in dfcity.iterrows():\n",
    "        # Total count of observation made by each users\n",
    "        user_obs = row['user_id']\n",
    "        n_obs[user_obs] += 1 # Increment observation count for the user\n",
    "        \n",
    "        # Total identification user made\n",
    "        identifications = row['identifications']\n",
    "        if not pd.isnull(identifications):  # Check if 'identifications' is not NaN\n",
    "            identifications = eval(identifications)\n",
    "            for ident in identifications:\n",
    "                user_ident = ident['user_id']\n",
    "                if user_ident != user_obs:\n",
    "                    n_idents[user_ident] += 1 # Increment identification count for the user\n",
    "    \n",
    "    # Aggregating results into a list of dictionaries\n",
    "    obs_ident_list = []\n",
    "    for user_id, obs_count in n_obs.items():\n",
    "        obs_ident_list.append({\n",
    "            'user_id': user_id,\n",
    "            'n_obs': obs_count,\n",
    "            'n_idents': n_idents[user_id],\n",
    "            #'day_type': row['day_type'],  # Include the 'day_type' column in the result\n",
    "            'year': year,\n",
    "            'time_observed_at': row['time_observed_at']\n",
    "            \n",
    "            \n",
    "        })\n",
    "    \n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    dfobsident = pd.DataFrame(obs_ident_list)\n",
    "    \n",
    "    return dfobsident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf469da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each city's data and concatenate the results\n",
    "dfobsident_list = []\n",
    "for city, years in city_years.items():\n",
    "    for year in years:\n",
    "        dfobsident = get_obs_ident_per_users(dfs[city][year], year)\n",
    "        dfobsident_list.append(dfobsident)\n",
    "\n",
    "# Concatenate all observation-identification dataframes into a single dataframe\n",
    "dfobsident = pd.concat(dfobsident_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfobsident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7029397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dictionary to store results for each year\n",
    "sf_dataframes = {}\n",
    "\n",
    "# Iterate over the years\n",
    "for year in range(2016, 2024):\n",
    "    # Extract DataFrame for the current year and city 'SF'\n",
    "    dfcity = dfs['Francisco_Bay'][year]\n",
    "    # Process data for the current year\n",
    "    sf_dataframes[year] = get_obs_ident_per_users(dfcity, year)\n",
    "\n",
    "#access dataframe for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207ff35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_2016 = sf_dataframes[2016]\n",
    "sf_2017 = sf_dataframes[2017]\n",
    "sf_2018 = sf_dataframes[2018]\n",
    "sf_2019 = sf_dataframes[2019]\n",
    "sf_2020 = sf_dataframes[2020]\n",
    "sf_2021 = sf_dataframes[2021]\n",
    "sf_2022 = sf_dataframes[2022]\n",
    "sf_2023 = sf_dataframes[2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233f7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    # Step before log transformation to avoid the zeros\n",
    "    df['obs_count_1'] = df['n_obs'] + 1\n",
    "    df['ident_count_1'] = df['n_idents'] + 1\n",
    "\n",
    "    # Log transform\n",
    "    df['obs_count_log'] = np.log(df['obs_count_1'])\n",
    "    df['ident_count_log'] = np.log(df['ident_count_1'])\n",
    "\n",
    "    # Select variables for plotting\n",
    "    df_select = df[['user_id', 'obs_count_log', 'ident_count_log']]\n",
    "\n",
    "    # Create a copy of the dataframe\n",
    "    df_std = df_select.copy()\n",
    "\n",
    "    # Don't include the user_id column in the transformation\n",
    "    col_names = ['obs_count_log', 'ident_count_log']\n",
    "    features = df_std[col_names]\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    features = scaler.transform(features.values)\n",
    "\n",
    "    # Assign the result to those two columns\n",
    "    df_std[col_names] = features\n",
    "    df_std.rename(columns={'obs_count_log': 'obs_count_scaled', 'ident_count_log': 'ident_count_scaled'}, inplace=True)\n",
    "    df_std.sort_values('user_id', inplace=True)\n",
    "    \n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa7287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transform_data(dfobsident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b91ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a ratio for splitting the digits list into a training and a validation set.\n",
    "#digits = list(digits)\n",
    "#extract features/ column for clustering\n",
    "data = list(zip(df_transformed['obs_count_scaled'], df_transformed['ident_count_scaled']))\n",
    "\n",
    "ratio = int(len(data)*0.25)\n",
    "validation = data[:ratio] #Validation set\n",
    "training = data[ratio:] #training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce1f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_centroids(data, k):\n",
    "    \"\"\"\n",
    "    Randomly pick some k centers from the data as starting values for centroids.\n",
    "    \"\"\"\n",
    "    return random.sample(data, k)\n",
    "\n",
    "# Define a function to sum a cluster element-wise\n",
    "def sum_cluster(cluster):\n",
    "    \"\"\"\n",
    "    Element-wise sums a list of arrays.\n",
    "    \"\"\"\n",
    "    if len(cluster) == 0:\n",
    "        return np.zeros_like(cluster[0])  # Return a zero vector if the cluster is empty\n",
    "    sum_ = np.array(cluster[0]).copy()\n",
    "    for vector in cluster[1:]:\n",
    "        sum_ += vector\n",
    "    return sum_\n",
    "\n",
    "# Define a function to compute the mean of a cluster\n",
    "def mean_cluster(cluster):\n",
    "    \"\"\"\n",
    "    Computes the mean (i.e., the centroid at the middle) of a list of vectors (a cluster).\n",
    "    Takes the sum and then divides by the size of the cluster.\n",
    "    \"\"\"\n",
    "    if len(cluster) == 0:\n",
    "        return np.zeros(len(cluster[0]))  # Return a zero vector if the cluster is empty\n",
    "    sum_of_points = sum_cluster(cluster)\n",
    "    mean_of_points = sum_of_points * (1.0 / len(cluster))\n",
    "    return mean_of_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fd5142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to form clusters\n",
    "def form_clusters(data, centroids):\n",
    "    \"\"\"\n",
    "    Given some data and centroids for the data, allocate each datapoint\n",
    "    to its closest centroid. This forms clusters.\n",
    "    \"\"\"\n",
    "    centroids_indices = range(len(centroids))\n",
    "    clusters = {c: [] for c in centroids_indices}\n",
    "    \n",
    "    for point in data:\n",
    "        smallest_distance = float(\"inf\")\n",
    "        for cj_index in centroids_indices:\n",
    "            cj = centroids[cj_index]\n",
    "            distance = np.linalg.norm(np.array(point) - cj)\n",
    "            if distance < smallest_distance:\n",
    "                closest_centroid_index = cj_index\n",
    "                smallest_distance = distance\n",
    "        clusters[closest_centroid_index].append(point)\n",
    "    \n",
    "    return list(clusters.values())\n",
    "\n",
    "\n",
    "def move_centroids(clusters, data):\n",
    "    \"\"\"\n",
    "    Returns a list of centroids corresponding to the clusters.\n",
    "    If a cluster is empty, reinitialize its centroid to a random data point.\n",
    "    \"\"\"\n",
    "    new_centroids = []\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) > 0:\n",
    "            new_centroids.append(mean_cluster(cluster))\n",
    "        else:\n",
    "            print(\"Warning: Empty cluster detected. Reinitializing to a random data point.\")\n",
    "            new_centroids.append(random.choice(data))\n",
    "    return new_centroids\n",
    "\n",
    "# Define a function to repeat until convergence\n",
    "def repeat_until_convergence(data, clusters, centroids):\n",
    "    \"\"\"\n",
    "    Form clusters around centroids, then keep moving the centroids\n",
    "    until the moves are no longer significant, i.e., we've found\n",
    "    the best-fitting centroids for the data.\n",
    "    \"\"\"\n",
    "    previous_max_difference = float(\"inf\")\n",
    "    while True:\n",
    "        old_centroids = centroids\n",
    "        centroids = move_centroids(clusters, data)\n",
    "        clusters = form_clusters(data, centroids)\n",
    "        \n",
    "        differences = [np.linalg.norm(np.array(a) - np.array(b)) for a, b in zip(old_centroids, centroids)]\n",
    "        max_difference = max(differences)\n",
    "        mean_difference = np.mean([previous_max_difference, max_difference])\n",
    "        \n",
    "        if np.isnan(mean_difference) or mean_difference == 0:\n",
    "            difference_change = 0\n",
    "        else:\n",
    "            difference_change = abs((max_difference - previous_max_difference) / mean_difference) * 100\n",
    "        \n",
    "        previous_max_difference = max_difference\n",
    "        \n",
    "        if difference_change == 0:\n",
    "            break\n",
    "    \n",
    "    return clusters, centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53fdd812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to run k-means clustering\n",
    "def cluster(data, k):\n",
    "    \"\"\"\n",
    "    Runs k-means clustering on the data.\n",
    "    \"\"\"\n",
    "    centroids = init_centroids(data, k)\n",
    "    clusters = form_clusters(data, centroids)\n",
    "    final_clusters, final_centroids = repeat_until_convergence(data, clusters, centroids)\n",
    "    return final_clusters, final_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d373ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to assign labels to centroids\n",
    "def assign_labels_to_centroids(clusters, centroids):\n",
    "    \"\"\"\n",
    "    Assigns a label to each cluster. In this context, the label can be the centroid itself.\n",
    "    NOTE: this function depends on clusters and centroids being in the same order.\n",
    "    \"\"\"\n",
    "    labelled_centroids = [(centroids[i], clusters[i]) for i in range(len(clusters))]\n",
    "    return labelled_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c9b0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_user(user, labelled_centroids):\n",
    "    \"\"\"\n",
    "    Classifies a user based on the nearest centroid.\n",
    "    \"\"\"\n",
    "    smallest_distance = float('inf')\n",
    "    closest_label = None\n",
    "    \n",
    "    for label, centroid in labelled_centroids:\n",
    "        distance = np.linalg.norm(np.array(user) - np.array(centroid))\n",
    "        if distance < smallest_distance:\n",
    "            smallest_distance = distance\n",
    "            closest_label = label\n",
    "    \n",
    "    return closest_label\n",
    "\n",
    "\n",
    "def get_error_rate(users, labelled_centroids):\n",
    "    classified_incorrect = 0\n",
    "    \n",
    "    for user in users:\n",
    "        classified_label = classify_user(user, labelled_centroids)\n",
    "        if not any(np.array_equal(classified_label, c[0]) for c in labelled_centroids):\n",
    "            classified_incorrect += 1\n",
    "    error_rate = classified_incorrect / float(len(users))\n",
    "    return error_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## recheck this\n",
    "\n",
    "def get_error_rate(users, labelled_centroids):\n",
    "    classified_incorrect = 0\n",
    "    \n",
    "    # Convert centroids to tuples \n",
    "    labelled_centroids_tuples = [(label, tuple(centroid)) for label, centroid in labelled_centroids]\n",
    "    \n",
    "    for user in users:\n",
    "        classified_label = classify_user(user, labelled_centroids)\n",
    "        \n",
    "        # Debug print to check classified labels and centroids\n",
    "        print(f\"User: {user}, Classified Label: {classified_label}\")\n",
    "        \n",
    "        if not any(classified_label == c[0] for c in labelled_centroids_tuples):\n",
    "            classified_incorrect += 1\n",
    "    \n",
    "    error_rate = classified_incorrect / float(len(users))\n",
    "    \n",
    "    # Debug print to check the error rate\n",
    "    print(f\"Classified Incorrect: {classified_incorrect}, Total Users: {len(users)}, Error Rate: {error_rate}\")\n",
    "    \n",
    "    return error_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05444bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/w5myjh6j1wl3m52tqz_3fmww0000gn/T/ipykernel_5720/4144908857.py:57: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  difference_change = abs((max_difference - previous_max_difference) / mean_difference) * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n",
      "Warning: Empty cluster detected. Reinitializing to a random data point.\n"
     ]
    }
   ],
   "source": [
    "# Initialize error rates dictionary\n",
    "error_rates = {x: None for x in list(range(5, 25)) + [100]}\n",
    "\n",
    "# Calculate error rates for each value of k\n",
    "for k in range(5, 25):\n",
    "    trained_clusters, trained_centroids = cluster(training, k)\n",
    "    labelled_centroids = assign_labels_to_centroids(trained_clusters, trained_centroids)\n",
    "    error_rate = get_error_rate(validation, labelled_centroids)\n",
    "    error_rates[k] = error_rate\n",
    "\n",
    "# Also calculate for k=100\n",
    "trained_clusters, trained_centroids = cluster(training, 100)\n",
    "labelled_centroids = assign_labels_to_centroids(trained_clusters, trained_centroids)\n",
    "error_rate = get_error_rate(validation, labelled_centroids)\n",
    "error_rates[100] = error_rate\n",
    "\n",
    "# Show the error rates\n",
    "x_axis = sorted(error_rates.keys())\n",
    "y_axis = [error_rates[key] for key in x_axis]\n",
    "plt.figure()\n",
    "plt.title(\"Error Rate by Number of Clusters\")\n",
    "plt.scatter(x_axis, y_axis)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4765e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff02f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab962ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eed5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352e630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6efb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d499b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6885908",
   "metadata": {},
   "source": [
    "## Originial code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing Lloyd's Algorithm for K-Means clustering.\n",
    "# (This exists in various libraries, but it's good practice to write by hand.)\n",
    "def init_centroids(labelled_data, k):\n",
    "    \"\"\"\n",
    "    Randomly pick some k centers from the data as starting values for centroids.\n",
    "    Remove labels.\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: x[1], random.sample(labelled_data, k)))\n",
    "\n",
    "# Define a function to sum a cluster element-wise\n",
    "def sum_cluster(labelled_cluster):\n",
    "    \"\"\"\n",
    "    Element-wise sums a list of arrays. Assumes all datapoints in labelled_cluster are labelled.\n",
    "    \"\"\"\n",
    "    sum_ = labelled_cluster[0][1].copy()\n",
    "    for (label, vector) in labelled_cluster[1:]:\n",
    "        sum_ += vector\n",
    "    return sum_\n",
    "\n",
    "# Define a function to compute the mean of a cluster\n",
    "def mean_cluster(labelled_cluster):\n",
    "    \"\"\"\n",
    "    Computes the mean (i.e., the centroid at the middle) of a list of vectors (a cluster).\n",
    "    Takes the sum and then divides by the size of the cluster.\n",
    "    Assumes all datapoints in labelled_cluster are labelled.\n",
    "    \"\"\"\n",
    "    sum_of_points = sum_cluster(labelled_cluster)\n",
    "    mean_of_points = sum_of_points * (1.0 / len(labelled_cluster))\n",
    "    return mean_of_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to form clusters\n",
    "def form_clusters(labelled_data, unlabelled_centroids):\n",
    "    \"\"\"\n",
    "    Given some data and centroids for the data, allocate each datapoint\n",
    "    to its closest centroid. This forms clusters.\n",
    "    \"\"\"\n",
    "    centroids_indices = range(len(unlabelled_centroids))\n",
    "    clusters = {c: [] for c in centroids_indices}\n",
    "    \n",
    "    for (label, Xi) in labelled_data:\n",
    "        smallest_distance = float(\"inf\")\n",
    "        for cj_index in centroids_indices:\n",
    "            cj = unlabelled_centroids[cj_index]\n",
    "            distance = np.linalg.norm(Xi - cj)\n",
    "            if distance < smallest_distance:\n",
    "                closest_centroid_index = cj_index\n",
    "                smallest_distance = distance\n",
    "        clusters[closest_centroid_index].append((label, Xi))\n",
    "    \n",
    "    return list(clusters.values())\n",
    "\n",
    "# Define a function to move centroids\n",
    "def move_centroids(labelled_clusters):\n",
    "    \"\"\"\n",
    "    Returns a list of centroids corresponding to the clusters.\n",
    "    \"\"\"\n",
    "    new_centroids = []\n",
    "    for cluster in labelled_clusters:\n",
    "        new_centroids.append(mean_cluster(cluster))\n",
    "    return new_centroids\n",
    "\n",
    "# Define a function to repeat until convergence\n",
    "def repeat_until_convergence(labelled_data, labelled_clusters, unlabelled_centroids):\n",
    "    \"\"\"\n",
    "    Form clusters around centroids, then keep moving the centroids\n",
    "    until the moves are no longer significant, i.e., we've found\n",
    "    the best-fitting centroids for the data.\n",
    "    \"\"\"\n",
    "    previous_max_difference = 0\n",
    "    while True:\n",
    "        unlabelled_old_centroids = unlabelled_centroids\n",
    "        unlabelled_centroids = move_centroids(labelled_clusters)\n",
    "        labelled_clusters = form_clusters(labelled_data, unlabelled_centroids)\n",
    "        \n",
    "        differences = list(map(lambda a, b: np.linalg.norm(a - b), unlabelled_old_centroids, unlabelled_centroids))\n",
    "        max_difference = max(differences)\n",
    "        mean_difference = np.mean([previous_max_difference, max_difference])\n",
    "        \n",
    "        if mean_difference != 0:\n",
    "            difference_change = abs((max_difference - previous_max_difference) / mean_difference) * 100\n",
    "        else:\n",
    "            difference_change = 0\n",
    "        \n",
    "        previous_max_difference = max_difference\n",
    "        \n",
    "        if np.isnan(difference_change) or difference_change == 0:\n",
    "            break\n",
    "    \n",
    "    return labelled_clusters, unlabelled_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(labelled_data, k):\n",
    "    \"\"\"\n",
    "    runs k-means clustering on the data. It is assumed that the data is labelled.\n",
    "    \"\"\"\n",
    "    centroids = init_centroids(labelled_data, k)\n",
    "    clusters = form_clusters(labelled_data, centroids)\n",
    "    final_clusters, final_centroids = repeat_until_convergence(labelled_data, clusters, centroids)\n",
    "    return final_clusters, final_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels_to_centroids(clusters, centroids):\n",
    "    \"\"\"\n",
    "    Assigns a digit label to each cluster.\n",
    "    Cluster is a list of clusters containing labelled datapoints.\n",
    "    NOTE: this function depends on clusters and centroids being in the same order.\n",
    "    \"\"\"\n",
    "    labelled_centroids = []\n",
    "    for i in range(len(clusters)):\n",
    "        labels = list(map(lambda x: x[0], clusters[i]))  # Convert map to list\n",
    "        # Pick the most common label\n",
    "        most_common = max(set(labels), key=labels.count)\n",
    "        centroid = (most_common, centroids[i])\n",
    "        labelled_centroids.append(centroid)\n",
    "    return labelled_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_digit(digit, labelled_centroids):\n",
    "    \"\"\"\n",
    "    Given an unlabelled digit represented by a vector and a list of\n",
    "    labelled centroids [(label, vector)], determine the closest centroid\n",
    "    and thus classify the digit.\n",
    "    \"\"\"\n",
    "    mindistance = float(\"inf\")\n",
    "    for (label, centroid) in labelled_centroids:\n",
    "        distance = np.linalg.norm(centroid - digit)\n",
    "        if distance < mindistance:\n",
    "            mindistance = distance\n",
    "            closest_centroid_label = label\n",
    "    return closest_centroid_label\n",
    "\n",
    "def get_error_rate(digits, labelled_centroids):\n",
    "    \"\"\"\n",
    "    Classifies a list of labelled digits. Returns the error rate.\n",
    "    \"\"\"\n",
    "    classified_incorrect = 0\n",
    "    for (label, digit) in digits:\n",
    "        classified_label = classify_digit(digit, labelled_centroids)\n",
    "        if classified_label != label:\n",
    "            classified_incorrect += 1\n",
    "    error_rate = classified_incorrect / float(len(digits))\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbafc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize error rates dictionary\n",
    "error_rates = {x: None for x in list(range(5, 25)) + [100]}\n",
    "\n",
    "# Calculate error rates for each value of k\n",
    "for k in range(5, 25):\n",
    "    trained_clusters, trained_centroids = cluster(training, k)\n",
    "    labelled_centroids = assign_labels_to_centroids(trained_clusters, trained_centroids)\n",
    "    error_rate = get_error_rate(validation, labelled_centroids)\n",
    "    error_rates[k] = error_rate\n",
    "\n",
    "# Also calculate for k=100\n",
    "trained_clusters, trained_centroids = cluster(training, 100)\n",
    "labelled_centroids = assign_labels_to_centroids(trained_clusters, trained_centroids)\n",
    "error_rate = get_error_rate(validation, labelled_centroids)\n",
    "error_rates[100] = error_rate\n",
    "\n",
    "# Show the error rates\n",
    "x_axis = sorted(error_rates.keys())\n",
    "y_axis = [error_rates[key] for key in x_axis]\n",
    "plt.figure()\n",
    "plt.title(\"Error Rate by Number of Clusters\")\n",
    "plt.scatter(x_axis, y_axis)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f856e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the concatenation of range and list\n",
    "error_rates = {x: None for x in list(range(5, 25)) + [100]}\n",
    "\n",
    "for k in range(5, 25):\n",
    "    trained_clusters, trained_centroids = cluster(training, k)\n",
    "    labelled_centroids = assign_labels_to_centroids(trained_clusters, trained_centroids)\n",
    "    error_rate = get_error_rate(validation, labelled_centroids)\n",
    "    error_rates[k] = error_rate\n",
    "\n",
    "# Also calculate for k=100\n",
    "trained_clusters, trained_centroids = cluster(training, 100)\n",
    "labelled_centroids = assign_labels_to_centroids(trained_clusters, trained_centroids)\n",
    "error_rate = get_error_rate(validation, labelled_centroids)\n",
    "error_rates[100] = error_rate\n",
    "\n",
    "# Show the error rates\n",
    "x_axis = sorted(error_rates.keys())\n",
    "y_axis = [error_rates[key] for key in x_axis]\n",
    "plt.figure()\n",
    "plt.title(\"Error Rate by Number of Clusters\")\n",
    "plt.scatter(x_axis, y_axis)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f154e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
